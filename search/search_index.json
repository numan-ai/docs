{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Numan Docs","title":"Home"},{"location":"kb/","text":"Things about Knowledge Base","title":"About"},{"location":"literature/","text":"Hello","title":"Literature"},{"location":"literature/yago45/","text":"YAGO 4.5: A Large and Clean Knowldege Base with a Rich Taxonomy 1 INTRODUCTION 2 RELATED WORK Related KBs: ConceptNet, BabelNet, DBPedia, Freebase and Wikidata. Related ontologies: Cyc, SUMO, DOLCE, BFO, WordNet and Schema.org. We'll select Wikidata and Schema to build our KB. 3 DESIGNING YAGO 3.1 DESIGN RATIONALE We want to have a general, precise and non-redundant upper taxonomy. 3.2 DESIGN PRINCIPLES We want an upper taxonomy from Schema and a lower taxonomy from Wikidata, the integration is achieved through these principles: 1. PREFER PROPERTIES OVER CLASS MEMBERSHIP We prefer to do (A) -[has property]-> (B) Instead of (A) -[is of type]-> (B) Making A an instance of the B class. 2. CHOOSE THE PROPERTY WITH FEWER OBJECTS If we have some property like (C)<-(A)->(B) we prefer to do the inverse, (C)->(A) and (B)->(A) this way we have less objects per property. 3. THE UPPER TAXONOMY EXISTS TO DEFINE FORMAL PROPERTIES THAT WILL BE POPULATED All classes of upper taxonomy must define formal properties. Both the domain and range of these properties have to be upper-level classes. This makes the upper taxonomy a self contained schema so you don't need to search for property definitions in the lower classes. 4. THE LOWER TAXONOMY EXISTS TO CONVEY HUMAN-INTELLIGIBLE INFORMATION ABOUT ITS INSTANCES IN A NON-REDUNDANT FORM This targets mainly human users. The design principle tells us to remove classes that add no information over upper taxonomy classes, merge classes that are hard to distinguish and remove classes that aren't populated. 3.3 UPPER TAXONOMY How was the upper taxonomy built? UPPER-LEVEL CLASSES We started with the taxonomy of schema.org, excluding the classes: Action, BioChemEntity and MedicalEntity. All top-level classes are disjoint except places/organizations and products/creative works. FICTIONAL ENTITIES We add a class yago:FictionalEntity as a subclass of schema:Thing, by defining properties like \"createdBy\" and \"appearsIn\" its existence can be justified under Design Principle 1. Fictional entities are not disjoint from any other classes since everything can be fictional (Like a place, a person, an invention, etc.). INTANGIBLES We added the classes Award, Gender and BeliefSystem, all subclasses of schema:Intangible. Other subclasses of schema:Intangible were removed under Design Principle 3. PLACES Schema's taxonomy on places is very web page oriented, some classes don't define properties that could be populated so they're removed under Design Priniciple 3. Then, a new taxonomy was manually created like yago:HumanMadePlace and yago:AstronomicalObject. 3.4 LOWER TAXONOMY MAPPING TO WIKIDATA The lower levels of YAGO 4.5's taxonomy comes from Wikidata This mapping can give rise to the following situations: ONE-TO-ONE MAPPING (Upper class) -> (Wikidata class) ONE-TO-MANY MAPPING (Wikidata class) <- (Upper class) -> (Wikidata class) ONE-TO-NONE MAPPING This didn't happen in YAGO 4, it is now used for classes with a convoluted taxonomy in Wikidata It can be used for one of the following classes: - schema:Thing In YAGO 4 this used to be mapped to entity, which resulted in 1 million direct instances of schema:Thing. This goes against Design Principle 3. So now we only accept entities that fall into one of the manually approved subclasses of Thing. schema:Place Wikidata's taxonomy for places is highly convoluted, is difficult to tell apart terrain, geographical location, geographical region, geographical area and location. schema:Intangible Likewise, are highly convoluted: class, process or role. It's hard to establish properties to adhere to Design Principle 3. 3.5 INSTANCES Identifiers Every instance is automatically asigned a readable name, normally we use the title of the Wikipedia page. If there's none, we use the enlish label and concatenate it to the Wikidata Q-id. If there's none, we use a label that contains legal characters concatenated with the Wikidata id. Instances vs. Classes Wikidata contains items that are both Instances and Classes. Because of our discussion in 3.4, these items would be considered classes. At the same time, if we wanted to say that Eleanor Roosevelt spoke english, we would make a statement about the class, this statement is allowed thanks to OWL 2 by a mechanism called \"punning\". 4 IMPLEMENTING YAGO Infrastructure YAGO 4 was written in Rust but was hard to mantain so we moved to python. Data formats (Section that discusses formats of the project like BZ2, GZIP, NT, etc.) Data processing: All of this happens in a Unix machine with 90 CPUs and 800Gb of RAM Steps: 1.- Create schema | time < 1s Loads the schema 2.- Create taxonomy | time = 4hr Parses wikidata into classes and constructs a loop-free taxonomy 3.- Create facts | time = 4hr Parses wikidata into facts in the form of a YAGO predicate 4.- Type-check facts | time = 1.5hr The previous step outputed a list of facts, this list is now loaded into memory and each one is type-checked 5.- Create ids | time = 1hr All type-checked facts are now mapped to a YAGO name 6.- Create statistics | time = 1hr Counts things like number of instances per class, facts per predicate, etc. | Total time = 12hr 5 RESULT 5.1 RESOURCE Size YAGO has less predicates than wikidata deliberately according to Design Principle 3 but it does cover the 100 most frequent predicates YAGO 4.5 is smaller than YAGO 4 because of the removal of inverse properties according to Design Principle 2 Data Format: YAGO is split into these files: - Schema: Upper taxonomy, property definitions - Taxonomy: Hierarchies between classes as (a)-[is]->(b), better explained in General analysis.txt - Facts: All facts about entities that have an english wikipedia page - Beyond Wikipedia: Facts about entities that don't have that - Meta: Temporal annotations 5.2 EVALUATION table 2: | Criterion | Operationalization | Wikidata | YAGO 4 | YAGO 4.5 | | :--- | :--- | ---: | ---: | ---:| | Consistency | Absence of contradictions | no | yes | yes | | Complexity | Top-level classes | 41 | 2714 | 9 | | | Number of paths to root | 44 | 1.1 | 2.3 | | Modularity | Disjointness axioms | 0 | 18 | 24 | | Concisness | Taxonomic loops | 62 | 0 | 0 | | | Redundant taxonomic loops | 377k | 1216 | 0 | | | Reduntant relations | 118 | 6 | 0 | | | Classes without instances | 2.6M | 73 | 0 | | Understandabilty | Human-readable names | 0% | 89% | 91% | | Coverage | Classes per instance | 8.4 | 3.6 | 7.8 | | | Facts per instance | 4.8 | 5.1 | 2.7 | Intrinsic evaluation It is measured in this parameters: - Consistency: Absence of logical contradictions - Complexity: How complicated the ontology is, it's meassured by the number of top classes (i.e. All direct subclasses of schema:Thing) and the average number of paths from an instance to the taxonomy root - Modularity: How much is the ontology composed of discrete subsets, in this case, the subsets are disjoint classes (Not sure what a disjoint class is) - Conciseness: Absence of redundancies - Understandability: How comprehensible the ontology is, measured by the percentage of identifiers that have a human-raedable name - Coverage: The degree to which the ontology covers a certain domain of knowledge, measured by the number of classes and facts per instance Extrinsic evaluation We've attempted to solve ambiguous concepts. For each entity, it collects all candidates that share a label. Then it uses end-to-end entity linking system ExtEnD [6] that was pretrained on LongFormer [8]. 5.3 APPLICATIONS - BENCHMARKING YAGO has been widely used as benchmark datasets in entity type prediction [29] and link prediction [24] - YAGO IN INFORMATION RETRIEVAL For systems of information retrieval, YAGO has resulted useful in aiding with understanding queries and documents beyond the scope of word tokens and plain texts. It can help in two ways: Document retrieval: Finding relevant textual resources for a given user query Entity retrieval: Retrieves all entities from a document OTHER APPLICATIONS YAGO has been cited more tan 10,000 times according to Google Scholar AVAILABILITY Mentions YAGO's website and github page 6 CONCLUSION","title":"YAGO 4.5"},{"location":"literature/yago45/#yago-45-a-large-and-clean-knowldege-base-with-a-rich-taxonomy","text":"","title":"YAGO 4.5: A Large and Clean Knowldege Base with a Rich Taxonomy"},{"location":"literature/yago45/#1-introduction","text":"","title":"1 INTRODUCTION"},{"location":"literature/yago45/#2-related-work","text":"Related KBs: ConceptNet, BabelNet, DBPedia, Freebase and Wikidata. Related ontologies: Cyc, SUMO, DOLCE, BFO, WordNet and Schema.org. We'll select Wikidata and Schema to build our KB.","title":"2 RELATED WORK"},{"location":"literature/yago45/#3-designing-yago","text":"","title":"3 DESIGNING YAGO"},{"location":"literature/yago45/#31-design-rationale","text":"We want to have a general, precise and non-redundant upper taxonomy.","title":"3.1 DESIGN RATIONALE"},{"location":"literature/yago45/#32-design-principles","text":"We want an upper taxonomy from Schema and a lower taxonomy from Wikidata, the integration is achieved through these principles: 1. PREFER PROPERTIES OVER CLASS MEMBERSHIP We prefer to do (A) -[has property]-> (B) Instead of (A) -[is of type]-> (B) Making A an instance of the B class. 2. CHOOSE THE PROPERTY WITH FEWER OBJECTS If we have some property like (C)<-(A)->(B) we prefer to do the inverse, (C)->(A) and (B)->(A) this way we have less objects per property. 3. THE UPPER TAXONOMY EXISTS TO DEFINE FORMAL PROPERTIES THAT WILL BE POPULATED All classes of upper taxonomy must define formal properties. Both the domain and range of these properties have to be upper-level classes. This makes the upper taxonomy a self contained schema so you don't need to search for property definitions in the lower classes. 4. THE LOWER TAXONOMY EXISTS TO CONVEY HUMAN-INTELLIGIBLE INFORMATION ABOUT ITS INSTANCES IN A NON-REDUNDANT FORM This targets mainly human users. The design principle tells us to remove classes that add no information over upper taxonomy classes, merge classes that are hard to distinguish and remove classes that aren't populated.","title":"3.2 DESIGN PRINCIPLES"},{"location":"literature/yago45/#33-upper-taxonomy","text":"How was the upper taxonomy built?","title":"3.3 UPPER TAXONOMY"},{"location":"literature/yago45/#upper-level-classes","text":"We started with the taxonomy of schema.org, excluding the classes: Action, BioChemEntity and MedicalEntity. All top-level classes are disjoint except places/organizations and products/creative works.","title":"UPPER-LEVEL CLASSES"},{"location":"literature/yago45/#fictional-entities","text":"We add a class yago:FictionalEntity as a subclass of schema:Thing, by defining properties like \"createdBy\" and \"appearsIn\" its existence can be justified under Design Principle 1. Fictional entities are not disjoint from any other classes since everything can be fictional (Like a place, a person, an invention, etc.).","title":"FICTIONAL ENTITIES"},{"location":"literature/yago45/#intangibles","text":"We added the classes Award, Gender and BeliefSystem, all subclasses of schema:Intangible. Other subclasses of schema:Intangible were removed under Design Principle 3.","title":"INTANGIBLES"},{"location":"literature/yago45/#places","text":"Schema's taxonomy on places is very web page oriented, some classes don't define properties that could be populated so they're removed under Design Priniciple 3. Then, a new taxonomy was manually created like yago:HumanMadePlace and yago:AstronomicalObject.","title":"PLACES"},{"location":"literature/yago45/#34-lower-taxonomy","text":"MAPPING TO WIKIDATA The lower levels of YAGO 4.5's taxonomy comes from Wikidata This mapping can give rise to the following situations: ONE-TO-ONE MAPPING (Upper class) -> (Wikidata class) ONE-TO-MANY MAPPING (Wikidata class) <- (Upper class) -> (Wikidata class) ONE-TO-NONE MAPPING This didn't happen in YAGO 4, it is now used for classes with a convoluted taxonomy in Wikidata It can be used for one of the following classes: - schema:Thing In YAGO 4 this used to be mapped to entity, which resulted in 1 million direct instances of schema:Thing. This goes against Design Principle 3. So now we only accept entities that fall into one of the manually approved subclasses of Thing. schema:Place Wikidata's taxonomy for places is highly convoluted, is difficult to tell apart terrain, geographical location, geographical region, geographical area and location. schema:Intangible Likewise, are highly convoluted: class, process or role. It's hard to establish properties to adhere to Design Principle 3.","title":"3.4 LOWER TAXONOMY"},{"location":"literature/yago45/#35-instances","text":"Identifiers Every instance is automatically asigned a readable name, normally we use the title of the Wikipedia page. If there's none, we use the enlish label and concatenate it to the Wikidata Q-id. If there's none, we use a label that contains legal characters concatenated with the Wikidata id. Instances vs. Classes Wikidata contains items that are both Instances and Classes. Because of our discussion in 3.4, these items would be considered classes. At the same time, if we wanted to say that Eleanor Roosevelt spoke english, we would make a statement about the class, this statement is allowed thanks to OWL 2 by a mechanism called \"punning\".","title":"3.5 INSTANCES"},{"location":"literature/yago45/#4-implementing-yago","text":"","title":"4 IMPLEMENTING YAGO"},{"location":"literature/yago45/#infrastructure","text":"YAGO 4 was written in Rust but was hard to mantain so we moved to python.","title":"Infrastructure"},{"location":"literature/yago45/#data-formats","text":"(Section that discusses formats of the project like BZ2, GZIP, NT, etc.)","title":"Data formats"},{"location":"literature/yago45/#data-processing","text":"All of this happens in a Unix machine with 90 CPUs and 800Gb of RAM Steps:","title":"Data processing:"},{"location":"literature/yago45/#1-create-schema-time-1s","text":"Loads the schema","title":"1.- Create schema | time &lt; 1s"},{"location":"literature/yago45/#2-create-taxonomy-time-4hr","text":"Parses wikidata into classes and constructs a loop-free taxonomy","title":"2.- Create taxonomy | time = 4hr"},{"location":"literature/yago45/#3-create-facts-time-4hr","text":"Parses wikidata into facts in the form of a YAGO predicate","title":"3.- Create facts | time = 4hr"},{"location":"literature/yago45/#4-type-check-facts-time-15hr","text":"The previous step outputed a list of facts, this list is now loaded into memory and each one is type-checked","title":"4.- Type-check facts | time = 1.5hr"},{"location":"literature/yago45/#5-create-ids-time-1hr","text":"All type-checked facts are now mapped to a YAGO name","title":"5.- Create ids | time = 1hr"},{"location":"literature/yago45/#6-create-statistics-time-1hr","text":"Counts things like number of instances per class, facts per predicate, etc. | Total time = 12hr","title":"6.- Create statistics | time = 1hr"},{"location":"literature/yago45/#5-result","text":"","title":"5 RESULT"},{"location":"literature/yago45/#51-resource","text":"Size YAGO has less predicates than wikidata deliberately according to Design Principle 3 but it does cover the 100 most frequent predicates YAGO 4.5 is smaller than YAGO 4 because of the removal of inverse properties according to Design Principle 2 Data Format: YAGO is split into these files: - Schema: Upper taxonomy, property definitions - Taxonomy: Hierarchies between classes as (a)-[is]->(b), better explained in General analysis.txt - Facts: All facts about entities that have an english wikipedia page - Beyond Wikipedia: Facts about entities that don't have that - Meta: Temporal annotations","title":"5.1 RESOURCE"},{"location":"literature/yago45/#52-evaluation","text":"table 2: | Criterion | Operationalization | Wikidata | YAGO 4 | YAGO 4.5 | | :--- | :--- | ---: | ---: | ---:| | Consistency | Absence of contradictions | no | yes | yes | | Complexity | Top-level classes | 41 | 2714 | 9 | | | Number of paths to root | 44 | 1.1 | 2.3 | | Modularity | Disjointness axioms | 0 | 18 | 24 | | Concisness | Taxonomic loops | 62 | 0 | 0 | | | Redundant taxonomic loops | 377k | 1216 | 0 | | | Reduntant relations | 118 | 6 | 0 | | | Classes without instances | 2.6M | 73 | 0 | | Understandabilty | Human-readable names | 0% | 89% | 91% | | Coverage | Classes per instance | 8.4 | 3.6 | 7.8 | | | Facts per instance | 4.8 | 5.1 | 2.7 |","title":"5.2 EVALUATION"},{"location":"literature/yago45/#intrinsic-evaluation","text":"It is measured in this parameters: - Consistency: Absence of logical contradictions - Complexity: How complicated the ontology is, it's meassured by the number of top classes (i.e. All direct subclasses of schema:Thing) and the average number of paths from an instance to the taxonomy root - Modularity: How much is the ontology composed of discrete subsets, in this case, the subsets are disjoint classes (Not sure what a disjoint class is) - Conciseness: Absence of redundancies - Understandability: How comprehensible the ontology is, measured by the percentage of identifiers that have a human-raedable name - Coverage: The degree to which the ontology covers a certain domain of knowledge, measured by the number of classes and facts per instance","title":"Intrinsic evaluation"},{"location":"literature/yago45/#extrinsic-evaluation","text":"We've attempted to solve ambiguous concepts. For each entity, it collects all candidates that share a label. Then it uses end-to-end entity linking system ExtEnD [6] that was pretrained on LongFormer [8].","title":"Extrinsic evaluation"},{"location":"literature/yago45/#53-applications","text":"","title":"5.3 APPLICATIONS"},{"location":"literature/yago45/#-benchmarking","text":"YAGO has been widely used as benchmark datasets in entity type prediction [29] and link prediction [24]","title":"- BENCHMARKING"},{"location":"literature/yago45/#-yago-in-information-retrieval","text":"For systems of information retrieval, YAGO has resulted useful in aiding with understanding queries and documents beyond the scope of word tokens and plain texts. It can help in two ways: Document retrieval: Finding relevant textual resources for a given user query Entity retrieval: Retrieves all entities from a document","title":"- YAGO IN INFORMATION RETRIEVAL"},{"location":"literature/yago45/#other-applications","text":"YAGO has been cited more tan 10,000 times according to Google Scholar","title":"OTHER APPLICATIONS"},{"location":"literature/yago45/#availability","text":"Mentions YAGO's website and github page","title":"AVAILABILITY"},{"location":"literature/yago45/#6-conclusion","text":"","title":"6 CONCLUSION"},{"location":"nns/","text":"Experimenting with Neural Networks Subjectivity is central to all knowledge. We manage to handle a lot of sensory information by simplifying it into basic ideas. Instead of thinking about every photon hitting our eyes, we think about the apple that reflected those photons. This way of simplifying helps us process information effectively. However, it also shows how our understanding is personal and subjective. We constantly adapt our behavioural knowledge to the new incoming situation - we \"fine-tune\" our brains in real-time. Now, let's recall that LLMs store their vast knowledge in the model weights, for example the \"70b\" part in \"llama3.1 70b\" means how many parameters the model has, essentially acting as an indicator of the knowledge capacity. Both behavioural and factual knowledge is stored in weights. And in order to update weights of the model we need to train it further, which is not something we can do on the fly because of the limitations of the backpropagation algorithm. In our approach we will move the knowledge from weights to an external memory that is stuctured as a graph.","title":"Motivation"},{"location":"nns/#experimenting-with-neural-networks","text":"Subjectivity is central to all knowledge. We manage to handle a lot of sensory information by simplifying it into basic ideas. Instead of thinking about every photon hitting our eyes, we think about the apple that reflected those photons. This way of simplifying helps us process information effectively. However, it also shows how our understanding is personal and subjective. We constantly adapt our behavioural knowledge to the new incoming situation - we \"fine-tune\" our brains in real-time. Now, let's recall that LLMs store their vast knowledge in the model weights, for example the \"70b\" part in \"llama3.1 70b\" means how many parameters the model has, essentially acting as an indicator of the knowledge capacity. Both behavioural and factual knowledge is stored in weights. And in order to update weights of the model we need to train it further, which is not something we can do on the fly because of the limitations of the backpropagation algorithm. In our approach we will move the knowledge from weights to an external memory that is stuctured as a graph.","title":"Experimenting with Neural Networks"},{"location":"nns/graphs/","text":"Adding Graphs to NNs In order for a neural network to work with a graph it needs to be able to address nodes in that graph. For example if we want to add a property to a node we need to know to which node we are adding it. In regular programs we do it via the node id. Node ids are categorical, meaning that if we have ids 1, 2, 7 all we can say is that they are distinct, distance betwen the numbers give us no information. In neural networks we can represent categorical values with one-hot encoding, but there is a better way. Let's say we have a graph with four nodes (A, B, C, D): And now we want our neural network to be able to point at one of them. We can do this by outputing two float numbers from 0 to 1. Let's say the model gave us numbers 0.45 and 0.65, we can use them to uniquely identify a node in the graph: We used a two-dimentional space for the graph, but we can add more axis by adding more floaing numbers to the model output. And we can also divide the 0-1 interval into more parts. Here we divided it into only two parts. Having 5 axis and diving each one into 10 parts will give us 10 ** 5 = 100,000 possible node indices. This approach is better because we want to store concepts in this graph and concepts can be related to each other. Indexing nodes with vectors allows to use vector Euclidean distance as a measure of concept similarity, meaning that if two concepts are located in a similar position, they have a similar meaning.","title":"Graph Neural Networks"},{"location":"nns/graphs/#adding-graphs-to-nns","text":"In order for a neural network to work with a graph it needs to be able to address nodes in that graph. For example if we want to add a property to a node we need to know to which node we are adding it. In regular programs we do it via the node id. Node ids are categorical, meaning that if we have ids 1, 2, 7 all we can say is that they are distinct, distance betwen the numbers give us no information. In neural networks we can represent categorical values with one-hot encoding, but there is a better way. Let's say we have a graph with four nodes (A, B, C, D): And now we want our neural network to be able to point at one of them. We can do this by outputing two float numbers from 0 to 1. Let's say the model gave us numbers 0.45 and 0.65, we can use them to uniquely identify a node in the graph: We used a two-dimentional space for the graph, but we can add more axis by adding more floaing numbers to the model output. And we can also divide the 0-1 interval into more parts. Here we divided it into only two parts. Having 5 axis and diving each one into 10 parts will give us 10 ** 5 = 100,000 possible node indices. This approach is better because we want to store concepts in this graph and concepts can be related to each other. Indexing nodes with vectors allows to use vector Euclidean distance as a measure of concept similarity, meaning that if two concepts are located in a similar position, they have a similar meaning.","title":"Adding Graphs to NNs"}]}